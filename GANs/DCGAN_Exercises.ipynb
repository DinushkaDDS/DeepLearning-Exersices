{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Convolutional GANs\r\n",
    "\r\n",
    "In this notebook, we'll build a GAN using convolutional layers in the generator and discriminator. This is called a Deep Convolutional GAN, or DCGAN for short. The DCGAN architecture was first explored last year and has seen impressive results in generating new images, you can read the [original paper here](https://arxiv.org/pdf/1511.06434.pdf).\r\n",
    "\r\n",
    "We'll be training DCGAN on the [Street View House Numbers](http://ufldl.stanford.edu/housenumbers/) (SVHN) dataset. These are color images of house numbers collected from Google street view. SVHN images are in color and much more variable than MNIST. \r\n",
    "\r\n",
    "![SVHN Examples](assets/SVHN_examples.png)\r\n",
    "\r\n",
    "So, we'll need a deeper and more powerful network. This is accomplished through using convolutional layers in the discriminator and generator. It's also necessary to use batch normalization to get the convolutional networks to train. The only real changes compared to what [you saw previously](https://github.com/udacity/deep-learning/tree/master/gan_mnist) are in the generator and discriminator, otherwise the rest of the implementation is the same."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "\r\n",
    "import pickle as pkl\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "from scipy.io import loadmat\r\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!mkdir data"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting the data\n",
    "\n",
    "Here you can download the SVHN dataset. Run the cell above and it'll download to your machine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from urllib.request import urlretrieve\r\n",
    "from os.path import isfile, isdir\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "data_dir = 'data/'\r\n",
    "\r\n",
    "if not isdir(data_dir):\r\n",
    "    raise Exception(\"Data directory doesn't exist!\")\r\n",
    "\r\n",
    "class DLProgress(tqdm):\r\n",
    "    last_block = 0\r\n",
    "\r\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\r\n",
    "        self.total = total_size\r\n",
    "        self.update((block_num - self.last_block) * block_size)\r\n",
    "        self.last_block = block_num\r\n",
    "\r\n",
    "if not isfile(data_dir + \"train_32x32.mat\"):\r\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN Training Set') as pbar:\r\n",
    "        urlretrieve(\r\n",
    "            'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\r\n",
    "            data_dir + 'train_32x32.mat',\r\n",
    "            pbar.hook)\r\n",
    "\r\n",
    "if not isfile(data_dir + \"test_32x32.mat\"):\r\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN Testing Set') as pbar:\r\n",
    "        urlretrieve(\r\n",
    "            'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',\r\n",
    "            data_dir + 'test_32x32.mat',\r\n",
    "            pbar.hook)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These SVHN files are `.mat` files typically used with Matlab. However, we can load them in with `scipy.io.loadmat` which we imported above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainset = loadmat(data_dir + 'train_32x32.mat')\r\n",
    "testset = loadmat(data_dir + 'test_32x32.mat')"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here I'm showing a small sample of the images. Each of these is 32x32 with 3 color channels (RGB). These are the real images we'll pass to the discriminator and what the generator will eventually fake."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx = np.random.randint(0, trainset['X'].shape[3], size=36)\r\n",
    "fig, axes = plt.subplots(6, 6, sharex=True, sharey=True, figsize=(5,5))\r\n",
    "\r\n",
    "for ii, ax in zip(idx, axes.flatten()):\r\n",
    "    ax.imshow(trainset['X'][:,:,:,ii], aspect='equal')\r\n",
    "    ax.xaxis.set_visible(False)\r\n",
    "    ax.yaxis.set_visible(False)\r\n",
    "plt.subplots_adjust(wspace=0, hspace=0)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we need to do a bit of preprocessing and getting the images into a form where we can pass batches to the network. First off, we need to rescale the images to a range of -1 to 1, since the output of our generator is also in that range. We also have a set of test and validation images which could be used if we're trying to identify the numbers in the images."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scale(x, feature_range=(-1, 1)):\r\n",
    "    # scale to (0, 1)\r\n",
    "    # x = ((x - x.min())/(255 - x.min()))\r\n",
    "    \r\n",
    "    # # scale to feature_range\r\n",
    "    # min, max = feature_range\r\n",
    "    # x = x * (max - min) + min\r\n",
    "\r\n",
    "    x = (x - 127.5) / 127.5\r\n",
    "\r\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Dataset:\r\n",
    "    def __init__(self, train, test, val_frac=0.1, shuffle=False, scale_func=None):\r\n",
    "        split_idx = int(len(test['y'])*(1 - val_frac))\r\n",
    "        self.test_x, self.valid_x = test['X'][:,:,:,:split_idx], test['X'][:,:,:,split_idx:]\r\n",
    "        self.test_y, self.valid_y = test['y'][:split_idx], test['y'][split_idx:]\r\n",
    "        self.train_x, self.train_y = train['X'], train['y']\r\n",
    "        \r\n",
    "        self.train_x = np.rollaxis(self.train_x, 3)\r\n",
    "        self.valid_x = np.rollaxis(self.valid_x, 3)\r\n",
    "        self.test_x = np.rollaxis(self.test_x, 3)\r\n",
    "        \r\n",
    "        if scale_func is None:\r\n",
    "            self.scaler = scale\r\n",
    "        else:\r\n",
    "            self.scaler = scale_func\r\n",
    "        self.shuffle = shuffle\r\n",
    "        \r\n",
    "    def batches(self, batch_size):\r\n",
    "        if self.shuffle:\r\n",
    "            idx = np.arange(len(dataset.train_x))\r\n",
    "            np.random.shuffle(idx)\r\n",
    "            self.train_x = self.train_x[idx]\r\n",
    "            self.train_y = self.train_y[idx]\r\n",
    "        \r\n",
    "        n_batches = len(self.train_y)//batch_size\r\n",
    "        for ii in range(0, len(self.train_y), batch_size):\r\n",
    "            x = self.train_x[ii:ii+batch_size]\r\n",
    "            y = self.train_y[ii:ii+batch_size]\r\n",
    "            \r\n",
    "            yield self.scaler(x), y"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generator\r\n",
    "\r\n",
    "Here we'll build the generator network. The input will be our noise vector `z` as before. Also as before, the output will be a $tanh$ output (therefore output in the range -1,+1), but this time with size 32x32 which is the size of our SVHN images.\r\n",
    "\r\n",
    "What's new here is we'll use convolutional layers to create our new images. The first layer is a fully connected layer which is reshaped into a deep and narrow layer, something like 4x4x1024 as in the original DCGAN paper. Then we use batch normalization and a leaky ReLU activation. Next is a transposed convolution where typically you'd halve the depth and double the width and height of the previous layer. Again, we use batch normalization and leaky ReLU. For each of these layers, the general scheme is convolution > batch norm > leaky ReLU.\r\n",
    "\r\n",
    "You keep stacking layers up like this until you get the final transposed convolution layer with shape 32x32x3. Below is the archicture used in the original DCGAN paper:\r\n",
    "<center><img src=\"assets/dcgan.png\", width=\"500\" height=\"200\"/></center>\r\n",
    "Note that the final layer here is 64x64x3, while for our SVHN dataset, we only want it to be 32x32x3. \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras import layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def generator(alpha=0.2):\r\n",
    "    model = tf.keras.Sequential()\r\n",
    "\r\n",
    "    # Input to 3D cube reshape\r\n",
    "    model.add(layers.Dense(4*4*512, input_shape=(100,)))\r\n",
    "    model.add(layers.Reshape((4,4,512)))\r\n",
    "    model.add(layers.BatchNormalization())\r\n",
    "    model.add(layers.LeakyReLU(alpha))\r\n",
    "\r\n",
    "    model.add(layers.Conv2DTranspose(256, (5,5), strides=(2,2), padding=\"same\"))\r\n",
    "    model.add(layers.BatchNormalization())\r\n",
    "    model.add(layers.LeakyReLU(alpha))\r\n",
    "    \r\n",
    "    model.add(layers.Conv2DTranspose(64, (4,4), strides=(2,2), padding=\"same\"))\r\n",
    "    model.add(layers.BatchNormalization())\r\n",
    "    model.add(layers.LeakyReLU(alpha))\r\n",
    "\r\n",
    "    model.add(layers.Conv2DTranspose(32, (4,4), strides=(2,2), padding=\"same\"))\r\n",
    "    model.add(layers.BatchNormalization())\r\n",
    "    model.add(layers.LeakyReLU(alpha))\r\n",
    "\r\n",
    "    model.add(layers.Conv2DTranspose(8, (3,3), strides=(1,1), padding=\"same\"))\r\n",
    "    model.add(layers.BatchNormalization())\r\n",
    "    model.add(layers.LeakyReLU(alpha))\r\n",
    "    \r\n",
    "    model.add(layers.Conv2DTranspose(3, (3,3), strides=(1,1), padding=\"same\", activation=\"tanh\"))\r\n",
    "    \r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "g = generator()\r\n",
    "# noise = tf.random.normal([1, 100])\r\n",
    "# generated_image = g(noise, training=False)\r\n",
    "\r\n",
    "# plt.imshow(np.array(generated_image[0]*127.5 + 127.5, dtype=np.int32))\r\n",
    "\r\n",
    "g.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discriminator\r\n",
    "\r\n",
    "Here we'll build the discriminator. This is basically just a convolutional classifier like you've built before. The input to the discriminator are 32x32x3 tensors/images. You'll want a few convolutional layers, then a fully connected layer for the output. For the depths of the convolutional layers I suggest starting with 16, 32, 64 filters in the first layer, then double the depth as you add layers. Note that in the DCGAN paper, they did all the downsampling using only strided convolutional layers with no maxpool layers.\r\n",
    "\r\n",
    "You'll also want to use batch normalization with `tf.layers.batch_normalization` on each layer except the first convolutional and output layers. Again, each layer should look something like convolution > batch norm > leaky ReLU.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def discriminator(alpha=0.2):\r\n",
    "\r\n",
    "    model = tf.keras.Sequential()\r\n",
    "\r\n",
    "    # Input to 3D cube reshape\r\n",
    "    model.add(layers.Conv2D(128, kernel_size=(5,5), input_shape=(32,32,3)))\r\n",
    "    model.add(layers.Conv2D(64, kernel_size=(5,5), strides=(2,2)))\r\n",
    "    model.add(layers.LeakyReLU(alpha))\r\n",
    "\r\n",
    "    model.add(layers.Conv2D(32, kernel_size=(5,5)))\r\n",
    "    model.add(layers.BatchNormalization())\r\n",
    "    model.add(layers.LeakyReLU(alpha))\r\n",
    "\r\n",
    "    model.add(layers.Conv2D(16, kernel_size=(5,5), strides=(2,2)))\r\n",
    "    model.add(layers.BatchNormalization())\r\n",
    "    model.add(layers.LeakyReLU(alpha))\r\n",
    "\r\n",
    "    model.add(layers.Flatten())\r\n",
    "    model.add(layers.Dense(1))\r\n",
    "    \r\n",
    "    \r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building the model\n",
    "\n",
    "Here we can use the functions we defined about to build the model as a class. This will make it easier to move the network around in our code since the nodes and operations in the graph are packaged in one object."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is a function for displaying generated images."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This method returns a helper function to compute cross entropy loss\r\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generator_loss(fake_output):\r\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def discriminator_loss(real_output, fake_output):\r\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\r\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\r\n",
    "    total_loss = real_loss + fake_loss\r\n",
    "    return total_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the data and train the network here\r\n",
    "dataset = Dataset(trainset, testset, scale_func=scale)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generator_optimizer = tf.keras.optimizers.Adam(0.00002, beta_1=0.6)\r\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(0.001)\r\n",
    "\r\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0001, beta_1=0.5)\r\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generator_model = generator()\r\n",
    "discriminator_model = discriminator()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 128\r\n",
    "epochs = 20\r\n",
    "\r\n",
    "losses = []\r\n",
    "samples = []\r\n",
    "\r\n",
    "for e in range(epochs):\r\n",
    "\r\n",
    "    for x, y in dataset.batches(batch_size):\r\n",
    "        \r\n",
    "        noise = tf.random.normal([batch_size, 100])  # Z input\r\n",
    "\r\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n",
    "\r\n",
    "            generated_images = generator_model(noise, training=True)\r\n",
    "\r\n",
    "            real_output = discriminator_model(x, training=True)\r\n",
    "            fake_output = discriminator_model(generated_images, training=True)\r\n",
    "\r\n",
    "            gen_loss = generator_loss(fake_output)\r\n",
    "            disc_loss = discriminator_loss(real_output, fake_output)\r\n",
    "\r\n",
    "        \r\n",
    "        gradients_of_generator = gen_tape.gradient(gen_loss, generator_model.trainable_variables)\r\n",
    "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator_model.trainable_variables)\r\n",
    "\r\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator_model.trainable_variables))\r\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_model.trainable_variables))\r\n",
    "        print(f\"Batch gen-loss={gen_loss} disc_loss={disc_loss}\")\r\n",
    "        losses.append((disc_loss, gen_loss))\r\n",
    "\r\n",
    "    noise = tf.random.normal([16, 100])\r\n",
    "    predictions = generator_model(noise, training=False)\r\n",
    "    print(f\"Batch gen-loss={gen_loss} disc_loss={disc_loss}\")\r\n",
    "    plt.imshow(np.array((predictions[-1]*127.5 + 127.5), dtype=np.int32))\r\n",
    "\r\n",
    "    samples.append(predictions.numpy())\r\n",
    "\r\n",
    "    # losses.append((disc_loss, gen_loss))\r\n",
    "\r\n",
    "    print(f\"Epoch {e+1} completed!\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots()\r\n",
    "losses = np.array(losses)\r\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\r\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\r\n",
    "plt.title(\"Training Losses\")\r\n",
    "plt.legend()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def view_samples(samples, nrows, ncols,  epoch=-1 ,figsize=(5,5)):\r\n",
    "    fig, axes = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols, \r\n",
    "                             sharey=True, sharex=True)\r\n",
    "    samples = samples[epoch]\r\n",
    "\r\n",
    "    for ax, img in zip(axes.flatten(), samples):\r\n",
    "        ax.axis('off')\r\n",
    "        # img = ((img - img.min())*255 / (img.max() - img.min())).astype(np.uint8)\r\n",
    "        img = np.array((img*127.5 + 127.5),dtype=np.int32)\r\n",
    "        ax.set_adjustable('box')\r\n",
    "        im = ax.imshow(img, aspect='equal')\r\n",
    "   \r\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\r\n",
    "    return fig, axes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_ = view_samples(samples, 4, 4, figsize=(8,8),  epoch=-1)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = plt.imshow(np.array((samples[-1][-1]*127.5 + 127.5),dtype=np.int32))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}