{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "tf.executing_eagerly()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later.\r\n",
    "\r\n",
    "Basically if you experimenting use the default eager execution method, since it is easy to develop and debug. But if you need fast execution times use graph based execution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensorflow Gradient Tape is useful when computing backpropagation. (interesting use case)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "w = tf.Variable([[1.0]])\r\n",
    "with tf.GradientTape() as tape:\r\n",
    "  loss = w * w\r\n",
    "  print(loss)\r\n",
    "\r\n",
    "grad = tape.gradient(loss, w)\r\n",
    "print(grad)  # W**2 derivative is 2*W and derivative at 1 is therefore 2."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lets create a neural network to classify MNIST dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Fetch and format the mnist data\r\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\r\n",
    "\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\r\n",
    "  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\r\n",
    "   tf.cast(mnist_labels,tf.int64)))\r\n",
    "dataset = dataset.shuffle(1000).batch(32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Build the model\r\n",
    "mnist_model = tf.keras.Sequential()\r\n",
    "\r\n",
    "mnist_model.add(tf.keras.layers.Conv2D(16,[3,3], activation='relu', input_shape=(None, None, 1)))\r\n",
    "mnist_model.add(tf.keras.layers.Conv2D(16,[3,3], activation='relu'))\r\n",
    "mnist_model.add(tf.keras.layers.GlobalAveragePooling2D())\r\n",
    "mnist_model.add(tf.keras.layers.Dense(10))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This below code is possible because of the EAGER EXECUTION provided by the tensorflow. This model is not trained, but we can perform the forward prapagation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for images,labels in dataset.take(1):\r\n",
    "  print(\"Logits (Last layer values): \", mnist_model(images[0:1]).numpy())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logits (Last layer values):  [[ 0.00631617 -0.00770625 -0.03640431 -0.04774543  0.00959853 -0.01174832\n",
      "   0.02219779  0.03650862 -0.03261926 -0.02151665]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\r\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
    "\r\n",
    "loss_history = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def train_step(images, labels):\r\n",
    "    with tf.GradientTape() as tape:\r\n",
    "        logits = mnist_model(images, training=True)  #Get the last layer output\r\n",
    "        \r\n",
    "        # Add asserts to check the shape of the output.\r\n",
    "        tf.debugging.assert_equal(logits.shape, (32, 10))\r\n",
    "\r\n",
    "        loss_value = loss_object(labels, logits) # Calculate the output error\r\n",
    "\r\n",
    "    loss_history.append(loss_value.numpy().mean())\r\n",
    "\r\n",
    "    grads = tape.gradient(loss_value, mnist_model.trainable_variables)  # Calculate the gradient of loss (error function) at trainable_variable values\r\n",
    "    optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def train(epochs):\r\n",
    "    for epoch in range(epochs):\r\n",
    "        for (batch, (images, labels)) in enumerate(dataset):\r\n",
    "            train_step(images, labels)\r\n",
    "        print ('Epoch {} finished'.format(epoch+1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train(epochs = 3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 finished\n",
      "Epoch 2 finished\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.plot(loss_history)\r\n",
    "plt.xlabel('Batch #')\r\n",
    "plt.ylabel('Loss [entropy]')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing  a Linear Model from Scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Linear(tf.keras.Model):\r\n",
    "    '''\r\n",
    "    A class which defines a linear model in the for Wx+B.\r\n",
    "    '''\r\n",
    "    def __init__(self):\r\n",
    "        super(Linear, self).__init__()\r\n",
    "        self.W = tf.Variable(5., name='weight')\r\n",
    "        self.B = tf.Variable(10., name='biase')\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        '''\r\n",
    "        Inputs can be a vector. Then the weights and biases with broadcast to form Wx+B\r\n",
    "        '''\r\n",
    "        return inputs * self.W + self.B"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# A toy dataset of points around 3 * x + 2\r\n",
    "NUM_EXAMPLES = 2000\r\n",
    "training_inputs = tf.random.normal([NUM_EXAMPLES])\r\n",
    "noise = tf.random.normal([NUM_EXAMPLES])\r\n",
    "training_outputs = training_inputs * 3 + 2 + noise\r\n",
    "\r\n",
    "# The loss function to be optimized\r\n",
    "def loss(model, inputs, targets):\r\n",
    "    error = model(inputs) - targets\r\n",
    "    return tf.reduce_mean(tf.square(error))\r\n",
    "\r\n",
    "def grad(model, inputs, targets):\r\n",
    "    with tf.GradientTape() as tape:\r\n",
    "        loss_value = loss(model, inputs, targets)\r\n",
    "    return tape.gradient(loss_value, [model.W, model.B])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Linear()\r\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\n",
    "\r\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\r\n",
    "\r\n",
    "steps = 300\r\n",
    "for i in range(steps):\r\n",
    "    grads = grad(model, training_inputs, training_outputs) #Calculating the gradient\r\n",
    "    optimizer.apply_gradients(zip(grads, [model.W, model.B]))   # Calculating the delta values and updating the variable (Backpropagation)\r\n",
    "    if i % 20 == 0:\r\n",
    "        print(f\"Loss at step {i}: {loss(model, training_inputs, training_outputs)} with W={model.W.numpy()} and B={model.B.numpy()}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}