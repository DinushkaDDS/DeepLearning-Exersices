{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Automatic Differenciation is very interesting concept which is used in calculation of partial derivatives etc for a variable. \r\n",
    "### Interesting, if possible learn more about this ..!\r\n",
    "\r\n",
    "#### https://www.tensorflow.org/guide/autodiff tensorflow implementation details...!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\r\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "x = tf.Variable(3.0)\r\n",
    "\r\n",
    "with tf.GradientTape() as tape:\r\n",
    "    y = x**2\r\n",
    "\r\n",
    "# dy = 2x * dx \r\n",
    "dy_dx = tape.gradient(y, x)\r\n",
    "dy_dx.numpy()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\r\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\r\n",
    "x = [[1., 2., 3.]]\r\n",
    "\r\n",
    "with tf.GradientTape(persistent=True) as tape:\r\n",
    "    y = x @ w + b   # matrix multiplication\r\n",
    "    loss = tf.reduce_mean(y**2)   # Mean Squared error\r\n",
    "\r\n",
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\r\n",
    "dl_db"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-1.1915586,  3.9254484], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In above instead of passing a variable list we can pass variable dictionary to the gradient function.\r\n",
    "\r\n",
    "eg: \r\n",
    "my_vars = {\r\n",
    "    'w': w,\r\n",
    "    'b': b\r\n",
    "}\r\n",
    "grad = tape.gradient(loss, my_vars)\r\n",
    "grad['b']"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\r\n",
    "x = tf.constant([[1., 2., 3.]])\r\n",
    "\r\n",
    "with tf.GradientTape() as tape:\r\n",
    "    # Forward pass\r\n",
    "    y = layer(x)\r\n",
    "    loss = tf.reduce_mean(y**2)\r\n",
    "\r\n",
    "# Calculate gradients with respect to every trainable variable\r\n",
    "grad = tape.gradient(loss, layer.trainable_variables)\r\n",
    "\r\n",
    "for var, g in zip(layer.trainable_variables, grad):\r\n",
    "    print(f'{var.name}, shape: {g.shape}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dense/kernel:0, shape: (3, 2)\n",
      "dense/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# A trainable variable\r\n",
    "x0 = tf.Variable(3.0, name='x0')\r\n",
    "# Not trainable\r\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\r\n",
    "# Not a Variable: A variable + tensor returns a tensor.\r\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\r\n",
    "# Not a variable\r\n",
    "x3 = tf.constant(3.0, name='x3')\r\n",
    "\r\n",
    "with tf.GradientTape() as tape:\r\n",
    "  y = (x0**2) + (x1**2) + (x2**2)\r\n",
    "\r\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\r\n",
    "\r\n",
    "for g in grad:\r\n",
    "    print(g)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "x0 = tf.Variable(0.0)\r\n",
    "x1 = tf.Variable(10.0)\r\n",
    "\r\n",
    "#Use below to override the default behaviour of watching the all variables and manually assign which variables to watch\r\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\r\n",
    "    tape.watch(x1)\r\n",
    "    y0 = tf.math.sin(x0)\r\n",
    "    y1 = tf.nn.softplus(x1)\r\n",
    "    y = y0 + y1\r\n",
    "    ys = tf.reduce_sum(y)\r\n",
    "    \r\n",
    "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\r\n",
    "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\r\n",
    "\r\n",
    "print('dy/dx0:', grad['x0'])\r\n",
    "print('dy/dx1:', grad['x1'].numpy())    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dy/dx0: None\n",
      "dy/dx1: 0.9999546\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### *** To compute multiple gradients over the same computation, create a gradient tape with persistent=True"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "x0 = tf.Variable(3.0)\r\n",
    "x1 = tf.Variable(0.0)\r\n",
    "\r\n",
    "with tf.GradientTape() as tape:\r\n",
    "    # Update x1 = x1 + x0.\r\n",
    "    x3 = x1.assign_add(x0)\r\n",
    "    # The tape starts recording from x1.\r\n",
    "    y = x3**2   # y = (x1 + x0)**2\r\n",
    "\r\n",
    "    print(type(x3))\r\n",
    "\r\n",
    "# This doesn't work.\r\n",
    "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'tensorflow.python.ops.resource_variable_ops._UnreadVariable'>\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}