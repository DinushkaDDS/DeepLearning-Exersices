{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, we'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import time\r\n",
    "from collections import namedtuple\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "with open('anna.txt', 'r') as f:\r\n",
    "    text=f.read()\r\n",
    "\r\n",
    "vocab = sorted(set(text))\r\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\r\n",
    "\r\n",
    "int_to_vocab = dict(enumerate(vocab))\r\n",
    "\r\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "text[:100]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we can see the characters encoded as integers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "encoded[:100]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
       "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
       "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70, 64,\n",
       "       57, 72, 72, 81,  1, 62, 57, 69, 65, 68, 81,  1, 65, 75,  1, 77, 70,\n",
       "       64, 57, 72, 72, 81,  1, 65, 70,  1, 65, 76, 75,  1, 71, 79, 70,  0,\n",
       "       79, 57, 81, 13,  0,  0, 33, 78, 61, 74, 81, 76, 64, 65, 70])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "len(vocab)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making training mini-batches\r\n",
    "\r\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\r\n",
    "\r\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\r\n",
    "\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "We start with our text encoded as integers in one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\r\n",
    "\r\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the total number of batches, $K$, we can make from the array `arr`, you divide the length of `arr` by the number of characters per batch. Once you know the number of batches, you can get the total number of characters to keep from `arr`, $N * M * K$.\r\n",
    "\r\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`batch_size` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$.\r\n",
    "\r\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the $N \\times (M * K)$ array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_batches(arr, batch_size, n_steps):\r\n",
    "    '''Create a generator that returns batches of size\r\n",
    "       batch_size x n_steps from arr.\r\n",
    "       \r\n",
    "       Arguments\r\n",
    "       ---------\r\n",
    "       arr: Array you want to make batches from\r\n",
    "       batch_size: Batch size, the number of sequences per batch\r\n",
    "       n_steps: Number of sequence steps per batch\r\n",
    "    '''\r\n",
    "    # Get the number of characters per batch and number of batches we can make\r\n",
    "    characters_per_batch = batch_size*n_steps\r\n",
    "    n_batches = len(arr)//characters_per_batch\r\n",
    "    \r\n",
    "    # Keep only enough characters to make full batches\r\n",
    "    arr = arr[:characters_per_batch*n_batches]\r\n",
    "    \r\n",
    "    # Reshape into batch_size rows\r\n",
    "    arr = arr.reshape((batch_size, n_steps*n_batches))\r\n",
    "    \r\n",
    "    for n in range(0, arr.shape[1], n_steps):\r\n",
    "        # The features\r\n",
    "        x = arr[:, n:n+n_steps]\r\n",
    "        # The targets, shifted by one\r\n",
    "        y_temp = arr[:, n+1:n+n_steps+1]\r\n",
    "        \r\n",
    "        # For the very last batch, y will be one character short at the end of \r\n",
    "        # the sequences which breaks things. To get around this, I'll make an \r\n",
    "        # array of the appropriate size first, of all zeros, then add the targets.\r\n",
    "        # This will introduce a small artifact in the last batch, but it won't matter.\r\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\r\n",
    "        y[:,:y_temp.shape[1]] = y_temp\r\n",
    "        yield x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "batches = get_batches(encoded, 10, 50)\r\n",
    "x, y = next(batches)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print('x\\n', x[:10, :10])\r\n",
    "print('\\ny\\n', y[:10, :10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x\n",
      " [[31 64 57 72 76 61 74  1 16  0]\n",
      " [ 1 57 69  1 70 71 76  1 63 71]\n",
      " [78 65 70 13  0  0  3 53 61 75]\n",
      " [70  1 60 77 74 65 70 63  1 64]\n",
      " [ 1 65 76  1 65 75 11  1 75 65]\n",
      " [ 1 37 76  1 79 57 75  0 71 70]\n",
      " [64 61 70  1 59 71 69 61  1 62]\n",
      " [26  1 58 77 76  1 70 71 79  1]\n",
      " [76  1 65 75 70  7 76 13  1 48]\n",
      " [ 1 75 57 65 60  1 76 71  1 64]]\n",
      "\n",
      "y\n",
      " [[64 57 72 76 61 74  1 16  0  0]\n",
      " [57 69  1 70 71 76  1 63 71 65]\n",
      " [65 70 13  0  0  3 53 61 75 11]\n",
      " [ 1 60 77 74 65 70 63  1 64 65]\n",
      " [65 76  1 65 75 11  1 75 65 74]\n",
      " [37 76  1 79 57 75  0 71 70 68]\n",
      " [61 70  1 59 71 69 61  1 62 71]\n",
      " [ 1 58 77 76  1 70 71 79  1 75]\n",
      " [ 1 65 75 70  7 76 13  1 48 64]\n",
      " [75 57 65 60  1 76 71  1 64 61]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[55 63 69 22  6 76 45  5 16 35]\n",
    " [ 5 69  1  5 12 52  6  5 56 52]\n",
    " [48 29 12 61 35 35  8 64 76 78]\n",
    " [12  5 24 39 45 29 12 56  5 63]\n",
    " [ 5 29  6  5 29 78 28  5 78 29]\n",
    " [ 5 13  6  5 36 69 78 35 52 12]\n",
    " [63 76 12  5 18 52  1 76  5 58]\n",
    " [34  5 73 39  6  5 12 52 36  5]\n",
    " [ 6  5 29 78 12 79  6 61  5 59]\n",
    " [ 5 78 69 29 24  5  6 52  5 63]]\n",
    "\n",
    "y\n",
    " [[63 69 22  6 76 45  5 16 35 35]\n",
    " [69  1  5 12 52  6  5 56 52 29]\n",
    " [29 12 61 35 35  8 64 76 78 28]\n",
    " [ 5 24 39 45 29 12 56  5 63 29]\n",
    " [29  6  5 29 78 28  5 78 29 45]\n",
    " [13  6  5 36 69 78 35 52 12 43]\n",
    " [76 12  5 18 52  1 76  5 58 52]\n",
    " [ 5 73 39  6  5 12 52 36  5 78]\n",
    " [ 5 29 78 12 79  6 61  5 59 63]\n",
    " [78 69 29 24  5  6 52  5 63 76]]\n",
    " ```\n",
    " although the exact numbers will be different. Check to make sure the data is shifted over one step for `y`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSTM Cell\r\n",
    "\r\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\r\n",
    "\r\n",
    "* https://zhuanlan.zhihu.com/p/58854907\r\n",
    "* https://colah.github.io/posts/2015-08-Understanding-LSTMs/\r\n",
    "\r\n",
    "\r\n",
    "Below, we implement the `build_lstm` function to create these LSTM cells"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from tensorflow.keras import layers\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras.layers import LSTMCell, StackedRNNCells\r\n",
    "\r\n",
    "inputs = layers.Input(shape=(None,))\r\n",
    "\r\n",
    "embed = layers.Embedding(len(vocab), 128)(inputs)\r\n",
    "\r\n",
    "\r\n",
    "cells = StackedRNNCells([LSTMCell(128, dropout=0.3) for _ in range(2)])\r\n",
    "lstm_layer = layers.RNN(cells, return_sequences=True)(embed)\r\n",
    "\r\n",
    "\r\n",
    "dense = layers.Dense(len(vocab), activation=None)(lstm_layer)\r\n",
    "\r\n",
    "model = keras.Model(inputs=inputs, outputs=dense, name=\"test\")\r\n",
    "\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"test\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         10624     \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (None, None, 128)         263168    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 83)          10707     \n",
      "=================================================================\n",
      "Total params: 284,499\n",
      "Trainable params: 284,499\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "\r\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\r\n",
    "\r\n",
    "epochs = 25\r\n",
    "batch_size = 100         # Sequences per batch\r\n",
    "num_steps = 50          # Number of sequence steps per batch\r\n",
    "lstm_size = 128         # Size of hidden layers in LSTMs\r\n",
    "learning_rate = 0.01    # Learning rate\r\n",
    "keep_prob = 0.5         # Dropout keep probability\r\n",
    "\r\n",
    "for e in range(epochs):\r\n",
    "    \r\n",
    "    for x, y in get_batches(encoded, batch_size, num_steps):\r\n",
    "        \r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            logits = model(x, training=True)\r\n",
    "            \r\n",
    "            y_one_hot = tf.one_hot(y, len(vocab))\r\n",
    "            # print(logits.get_shape(), y_one_hot.get_shape())\r\n",
    "            y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\r\n",
    "            \r\n",
    "            # Softmax cross entropy loss\r\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\r\n",
    "            loss = tf.reduce_mean(loss)\r\n",
    "\r\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\r\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n",
    "\r\n",
    "        # print(r'Training loss: {:.4f}... '.format(loss))\r\n",
    "\r\n",
    "    print ('Epoch {} finished with {} loss'.format(e+1, loss))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 finished with 1.616369366645813 loss\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"Anna_KaRNNA.model\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: Anna_KaRNNA.model\\assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Assets written to: Anna_KaRNNA.model\\assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sampling\r\n",
    "\r\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. \r\n",
    "\r\n",
    "Since getting the argmax gives a looping like situation, using a randomization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "string = \"The world is \"\r\n",
    "stringEnc = [vocab_to_int[i] for i in string]\r\n",
    "temp = model(np.array(stringEnc).reshape(1,len(string),1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "[int_to_vocab[np.argmax(temp[-1][-1].numpy())]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['t']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Not the Best way to do the prediction, but way easier, xD\r\n",
    "## Otherwise will need to keep the states of RNN layers save and pass them accordingly\r\n",
    "## This require additional programming like a class as far as I understand"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "string = \"What is going\"\r\n",
    "stringEnc = [vocab_to_int[i] for i in string]\r\n",
    "temp = model(np.array(stringEnc).reshape(1,len(string),1))\r\n",
    "\r\n",
    "char_index_to_append = np.argmax(temp[-1][-1].numpy())\r\n",
    "\r\n",
    "for i in range(1000):\r\n",
    "    stringEnc.append(char_index_to_append)\r\n",
    "\r\n",
    "    if(len(stringEnc)<50):\r\n",
    "        temp = model(np.array(stringEnc).reshape(1, len(stringEnc), 1))\r\n",
    "    else:\r\n",
    "        temp = model(np.array(stringEnc[-50:]).reshape(1, 50, 1))\r\n",
    "\r\n",
    "\r\n",
    "    predicted_ids = tf.random.categorical(temp[-1], num_samples=1)\r\n",
    "    char_index_to_append = tf.squeeze(predicted_ids, axis=-1).numpy()[-1]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in stringEnc:\r\n",
    "    print(int_to_vocab[i], end=\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "What is going bage,in when Anna Alexandrovna.\n",
      "That's instantated rushed into his scust\n",
      "was peasant for\n",
      "behes\n",
      "more to forgetting in an home,\" thought. Ything so thanked that; glad plopbatn. He left the\n",
      "Children.\n",
      "\n",
      "After the glad had siccote of\n",
      "the doess understood\n",
      "train\n",
      "man doing the fittle face, Anna Had to speak, this impress tighing to him. Yes, yes, Ko not emplaming his\n",
      "hand,\n",
      "has been\n",
      "they myself, that wonderedge.\n",
      "\n",
      "When\" (not biek to be continually break\n",
      "through at the\n",
      "baw and\n",
      "hands--and more Madaulny occulding, mistriculty guess, looked this painly, which he had been please and his\n",
      "brothings, that softary especially.\n",
      "\n",
      "\"Nikolaevitch his mother. She knew that the Gladent; Anna Levin. \"It was verbeve, nothress, with enem coming down, and then on\n",
      "him in\n",
      "a new grough and that they're noticing you, and,\n",
      "close--as Moscow that\n",
      "such are know his hand,\" said Kitty. The\n",
      "closed that he had Sergey\n",
      "Ivanovitch, with paint, and this long lew to his offering up to be understoryongful consequent: Turtation aps be"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "f1058ac39a4b5cc6a2d664bf07a90cc7a0b869b1d28e3e4a0289bda448411850"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}